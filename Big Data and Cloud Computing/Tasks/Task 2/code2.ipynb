{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scfLT2i0MLyD"
      },
      "source": [
        "# RAPIDS cuDF\n",
        "\n",
        "Material taken from [https://colab.research.google.com/drive/13sspqiEZwso4NYTbsflpPyNFaVAAxUgr].\n",
        "\n",
        "RAPIDS cuDF is preinstalled on Google Colab and instantly accelerates Pandas with zero code changes. This notebook template is for users who want to utilize the full suite of the RAPIDS libraries for their workflows on Colab.  \n",
        "\n",
        "# Environment Sanity Check #\n",
        "\n",
        "Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.\n",
        "\n",
        "You can check the output of `!nvidia-smi` to check which GPU you have.    Currently, RAPIDS runs on all available Colab GPU instances."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚠️ Verify your setup\n",
        "\n",
        "First, we'll verify that you are running with an NVIDIA GPU."
      ],
      "metadata": {
        "id": "SH_h6ci1Sx0u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2vPCtXcCvUR"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi  # this should display information about available GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_v33LnDVNo3"
      },
      "source": [
        "#Setup:\n",
        "This set up script:\n",
        "\n",
        "1. Checks to make sure that the GPU is RAPIDS compatible\n",
        "1. Installs the **current stable version** of RAPIDSAI's core libraries using pip, which are:\n",
        "  1. cuDF\n",
        "  1. cuML\n",
        "  1. cuGraph\n",
        "  1. cuSpatial\n",
        "  1. cuxFilter\n",
        "  1. cuCIM\n",
        "  1. xgboost\n",
        "\n",
        "**This will complete in about 5-6 minutes**\n",
        "\n",
        "If you require installing the **nightly** releases of RAPIDSAI, please use the [RAPIDS Conda Colab Template notebook](https://colab.research.google.com/drive/1TAAi_szMfWqRfHVfjGSqnGVLr_ztzUM9) and use the nightly parameter option when running the RAPIDS installation cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0C8IV5TQnjN"
      },
      "source": [
        "# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
        "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlsyk9m9NN2K"
      },
      "source": [
        "# Next Steps #\n",
        "\n",
        "For an overview of how you can access and work with your own datasets in Colab, check out [this guide](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92).\n",
        "\n",
        "For more RAPIDS examples, check out our RAPIDS notebooks repos:\n",
        "1. https://github.com/rapidsai/notebooks\n",
        "2. https://github.com/rapidsai/notebooks-contrib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZJMJ6BulmMn"
      },
      "source": [
        "# RAPIDS is now installed on Colab.  \n",
        "You can copy your code into the cells below or use the below to validate your RAPIDS installation and version.  \n",
        "# Enjoy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nLrk46BllED"
      },
      "source": [
        "import cudf\n",
        "cudf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cuml\n",
        "cuml.__version__"
      ],
      "metadata": {
        "id": "xgAFgI15ddf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cugraph\n",
        "cugraph.__version__"
      ],
      "metadata": {
        "id": "JOCMWaUal1fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cuspatial\n",
        "cuspatial.__version__"
      ],
      "metadata": {
        "id": "AnmtYjzvVTtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cuxfilter\n",
        "cuxfilter.__version__"
      ],
      "metadata": {
        "id": "CYjcARDFVWWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For now, we will work with cuDF and time dataframe operations using ordinary Pandas and cuDF."
      ],
      "metadata": {
        "id": "qvnRaqAjtpuP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 Minutes to RAPIDS cuDF's pandas accelerator mode (cudf.pandas)\n",
        "\n",
        "cuDF is a Python GPU DataFrame library (built on the Apache Arrow columnar memory format) for loading, joining, aggregating, filtering, and otherwise manipulating tabular data using a DataFrame style API in the style of pandas.\n",
        "\n",
        "cuDF now provides a pandas accelerator mode (`cudf.pandas`), allowing you to bring accelerated computing to your pandas workflows without requiring any code change.\n",
        "\n",
        "This notebook is a short introduction to `cudf.pandas`."
      ],
      "metadata": {
        "id": "kcF9ZWvjSybR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our GPU-enabled Colab runtime active, we're ready to go. cuDF is available by default in the GPU-enabled runtime.\n",
        "\n",
        "If you're interested in installing on other platforms, please visit https://rapids.ai/#quick-start to learn more."
      ],
      "metadata": {
        "id": "KP0oc3PboQDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cudf  # this should work without any errors"
      ],
      "metadata": {
        "id": "zhPt4Xj8THgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also install `plotly-express` for visualizing data.\n",
        "\n",
        "### Environment Note\n",
        "If you're not running this notebook on Colab, you may need to reload the webpage for the `plotly.express` visualizations to work correctly.\n"
      ],
      "metadata": {
        "id": "4iryQthjTMXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly-express"
      ],
      "metadata": {
        "id": "kzBF7A8qTM27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the data\n",
        "\n",
        "The data we'll be working with is the [Parking Violations Issued - Fiscal Year 2022](https://data.cityofnewyork.us/City-Government/Parking-Violations-Issued-Fiscal-Year-2022/7mxj-7a6y) dataset from NYC Open Data.\n",
        "\n",
        "We're downloading a copy of this dataset from an s3 bucket hosted by NVIDIA to provide faster download speeds. We'll start by downloading the data. This should take about 30 seconds.\n",
        "\n",
        "## Data License and Terms\n",
        "As this dataset originates from the NYC Open Data Portal, it's governed by their license and terms of use.\n",
        "\n",
        "### Are there restrictions on how I can use Open Data?\n",
        "\n",
        "> Open Data belongs to all New Yorkers. There are no restrictions on the use of Open Data. Refer to Terms of Use for more information.\n",
        "\n",
        "### [Terms of Use](https://opendata.cityofnewyork.us/overview/#termsofuse)\n",
        "\n",
        "> By accessing datasets and feeds available through NYC Open Data, the user agrees to all of the Terms of Use of NYC.gov as well as the Privacy Policy for NYC.gov. The user also agrees to any additional terms of use defined by the agencies, bureaus, and offices providing data. Public data sets made available on NYC Open Data are provided for informational purposes. The City does not warranty the completeness, accuracy, content, or fitness for any particular purpose or use of any public data set made available on NYC Open Data, nor are any such warranties to be implied or inferred with respect to the public data sets furnished therein.\n",
        "\n",
        "> The City is not liable for any deficiencies in the completeness, accuracy, content, or fitness for any particular purpose or use of any public data set, or application utilizing such data set, provided by any third party.\n",
        "\n",
        "> Submitting City Agencies are the authoritative source of data available on NYC Open Data. These entities are responsible for data quality and retain version control of data sets and feeds accessed on the Site. Data may be updated, corrected, or refreshed at any time."
      ],
      "metadata": {
        "id": "4zGUeWvcTbDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.rapids.ai/datasets/nyc_parking/nyc_parking_violations_2022.parquet"
      ],
      "metadata": {
        "id": "5EoQqNwsTqeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis using Standard Pandas\n",
        "\n",
        "First, let's use Pandas to read in some columns of the dataset:"
      ],
      "metadata": {
        "id": "hAvNFbYKWwti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "SLRleX9xWxqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read 5 columns data:\n",
        "df = pd.read_parquet(\n",
        "    \"nyc_parking_violations_2022.parquet\",\n",
        "    columns=[\"Registration State\", \"Violation Description\", \"Vehicle Body Type\", \"Issue Date\", \"Summons Number\"]\n",
        ")\n",
        "\n",
        "# view a random sample of 10 rows:\n",
        "df.sample(10)"
      ],
      "metadata": {
        "id": "OLatEi7rW0la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll try to answer a few questions using the data."
      ],
      "metadata": {
        "id": "m7qXNJU9W53D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Which parking violation is most commonly committed by vehicles from various U.S states?\n",
        "\n",
        "Each record in our dataset contains the state of registration of the offending vehicle, and the type of parking offence. Let's say we want to get the most common type of offence for vehicles registered in different states. We can do this in Pandas using a combination of [value_counts](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html) and [GroupBy.head](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.head.html):"
      ],
      "metadata": {
        "id": "VmkFv9ZUW37g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df[[\"Registration State\", \"Violation Description\"]]  # get only these two columns\n",
        " .value_counts()  # get the count of offences per state and per type of offence\n",
        " .groupby(\"Registration State\")  # group by state\n",
        " .head(1)  # get the first row in each group (the type of offence with the largest count)\n",
        " .sort_index()  # sort by state name\n",
        " .reset_index()\n",
        ")"
      ],
      "metadata": {
        "id": "bHXq-s_ZXOQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above uses [method chaining](https://tomaugspurger.net/posts/method-chaining/) to combine a series of operations into a single statement. You might find it useful to break the code up into multiple statements and inspect each of the intermediate results!"
      ],
      "metadata": {
        "id": "8lXF4v4SXRf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Which vehicle body types are most frequently involved in parking violations?\n",
        "\n",
        "We can also investigate which vehicle body types most commonly appear in parking violations"
      ],
      "metadata": {
        "id": "H7_9EmGyXUJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df\n",
        " .groupby([\"Vehicle Body Type\"])\n",
        " .agg({\"Summons Number\": \"count\"})\n",
        " .rename(columns={\"Summons Number\": \"Count\"})\n",
        " .sort_values([\"Count\"], ascending=False)\n",
        ")"
      ],
      "metadata": {
        "id": "d7Ax-u4TXZtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How do parking violations vary across days of the week?"
      ],
      "metadata": {
        "id": "VjFfQLZHXehM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weekday_names = {\n",
        "    0: \"Monday\",\n",
        "    1: \"Tuesday\",\n",
        "    2: \"Wednesday\",\n",
        "    3: \"Thursday\",\n",
        "    4: \"Friday\",\n",
        "    5: \"Saturday\",\n",
        "    6: \"Sunday\",\n",
        "}\n",
        "\n",
        "df[\"Issue Date\"] = df[\"Issue Date\"].astype(\"datetime64[ms]\")\n",
        "df[\"issue_weekday\"] = df[\"Issue Date\"].dt.weekday.map(weekday_names)\n",
        "\n",
        "df.groupby([\"issue_weekday\"])[\"Summons Number\"].count().sort_values()"
      ],
      "metadata": {
        "id": "s5_y9m_AXhIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like there are fewer violations on weekends, which makes sense! During the week, more people are driving in New York City."
      ],
      "metadata": {
        "id": "LDeYr6xkXiDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's time it!\n",
        "\n",
        "Loading and processing this data took a little time. Let's measure how long these pipelines take in Pandas:"
      ],
      "metadata": {
        "id": "JKBQcT64XlMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "df = pd.read_parquet(\n",
        "    \"nyc_parking_violations_2022.parquet\",\n",
        "    columns=[\"Registration State\", \"Violation Description\", \"Vehicle Body Type\", \"Issue Date\", \"Summons Number\"]\n",
        ")\n",
        "\n",
        "(df[[\"Registration State\", \"Violation Description\"]]\n",
        " .value_counts()\n",
        " .groupby(\"Registration State\")\n",
        " .head(1)\n",
        " .sort_index()\n",
        " .reset_index()\n",
        ")"
      ],
      "metadata": {
        "id": "mDpQhus-Xnfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "(df\n",
        " .groupby([\"Vehicle Body Type\"])\n",
        " .agg({\"Summons Number\": \"count\"})\n",
        " .rename(columns={\"Summons Number\": \"Count\"})\n",
        " .sort_values([\"Count\"], ascending=False)\n",
        ")"
      ],
      "metadata": {
        "id": "9Gw5TWH2Xqgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "weekday_names = {\n",
        "    0: \"Monday\",\n",
        "    1: \"Tuesday\",\n",
        "    2: \"Wednesday\",\n",
        "    3: \"Thursday\",\n",
        "    4: \"Friday\",\n",
        "    5: \"Saturday\",\n",
        "    6: \"Sunday\",\n",
        "}\n",
        "\n",
        "df[\"Issue Date\"] = df[\"Issue Date\"].astype(\"datetime64[ms]\")\n",
        "df[\"issue_weekday\"] = df[\"Issue Date\"].dt.weekday.map(weekday_names)\n",
        "\n",
        "df.groupby([\"issue_weekday\"])[\"Summons Number\"].count().sort_values()"
      ],
      "metadata": {
        "id": "BovQgNrpXr2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using cudf.pandas\n",
        "\n",
        "Now, let's re-run the Pandas code above with the `cudf.pandas` extension loaded.\n",
        "\n",
        "Typically, you should load the `cudf.pandas` extension as the first step in your notebook, before importing any modules. Here, we explicitly restart the kernel to simulate that behavior."
      ],
      "metadata": {
        "id": "VgAWS0yXXtGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().kernel.do_shutdown(restart=True)"
      ],
      "metadata": {
        "id": "hW5rUr2tXzUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cudf.pandas"
      ],
      "metadata": {
        "id": "NjvPsTlGZrW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet(\n",
        "    \"nyc_parking_violations_2022.parquet\",\n",
        "    columns=[\"Registration State\", \"Violation Description\", \"Vehicle Body Type\", \"Issue Date\", \"Summons Number\"]\n",
        ")\n",
        "\n",
        "(df[[\"Registration State\", \"Violation Description\"]]\n",
        " .value_counts()\n",
        " .groupby(\"Registration State\")\n",
        " .head(1)\n",
        " .sort_index()\n",
        " .reset_index()\n",
        ")"
      ],
      "metadata": {
        "id": "XL_u4l5gZJte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1: The cell above executes a series of operations. What operation(s) is(are)actually taking less time to execute using cuDF when compared with pandas?"
      ],
      "metadata": {
        "id": "deHwIFyIw5Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "(df\n",
        " .groupby([\"Vehicle Body Type\"])\n",
        " .agg({\"Summons Number\": \"count\"})\n",
        " .rename(columns={\"Summons Number\": \"Count\"})\n",
        " .sort_values([\"Count\"], ascending=False)\n",
        ")"
      ],
      "metadata": {
        "id": "BLWa8ed6d-pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "weekday_names = {\n",
        "    0: \"Monday\",\n",
        "    1: \"Tuesday\",\n",
        "    2: \"Wednesday\",\n",
        "    3: \"Thursday\",\n",
        "    4: \"Friday\",\n",
        "    5: \"Saturday\",\n",
        "    6: \"Sunday\",\n",
        "}\n",
        "\n",
        "df[\"Issue Date\"] = df[\"Issue Date\"].astype(\"datetime64[ms]\")\n",
        "df[\"issue_weekday\"] = df[\"Issue Date\"].dt.weekday.map(weekday_names)\n",
        "\n",
        "df.groupby([\"issue_weekday\"])[\"Summons Number\"].count().sort_values()"
      ],
      "metadata": {
        "id": "X6ASy4mPd_-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much faster! Operations that took 5-20 seconds can now potentially finish in just milliseconds without changing any code."
      ],
      "metadata": {
        "id": "FMUrf6iMeBdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Performance\n",
        "\n",
        "`cudf.pandas` provides profiling utilities to help you better understand performance. With these tools, you can identify which parts of your code ran on the GPU and which parts ran on the CPU.\n",
        "\n",
        "They're accessible in the `cudf.pandas` namespace since the `cudf.pandas` extension was loaded above with `load_ext cudf.pandas`.\n",
        "\n",
        "#### Colab Note\n",
        "If you're running in Colab, the first time you run use the profiler it may take 10+ seconds due to Colab's debugger interacting with the built-in Python function [sys.settrace](https://docs.python.org/3/library/sys.html#sys.settrace) that we use for profiling. For demo purposes, this isn't an issue. Just run the cell again.\n",
        "\n",
        "## Profiling Functionality\n",
        "\n",
        "We can generate a per-function and per-line profile:"
      ],
      "metadata": {
        "id": "00m6gUxqeGzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cudf.pandas.profile\n",
        "\n",
        "small_df = pd.DataFrame({'a': [0, 1, 2], 'b': [\"x\", \"y\", \"z\"]})\n",
        "small_df = pd.concat([small_df, small_df])\n",
        "\n",
        "axis = 0\n",
        "for i in range(0, 2):\n",
        "    small_df.min(axis=axis, numeric_only=True)\n",
        "    axis = 1\n",
        "\n",
        "counts = small_df.groupby(\"a\").b.count()"
      ],
      "metadata": {
        "id": "RFm22OWbeHF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%cudf.pandas.line_profile\n",
        "\n",
        "small_df = pd.DataFrame({'a': [0, 1, 2], 'b': [\"x\", \"y\", \"z\"]})\n",
        "small_df = pd.concat([small_df, small_df])\n",
        "\n",
        "axis = 0\n",
        "for i in range(0, 2):\n",
        "    small_df.min(axis=axis, numeric_only=True)\n",
        "    axis = 1\n",
        "\n",
        "counts = small_df.groupby(\"a\").b.count()"
      ],
      "metadata": {
        "id": "Syb-_vZweN2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Behind the scenes: What's going on here?\n",
        "\n",
        "When you load `cudf.pandas`, Pandas types like `Series` and `DataFrame` are replaced by proxy objects that dispatch operations to cuDF when possible. We can verify that `cudf.pandas` is active by looking at our `pd` variable:"
      ],
      "metadata": {
        "id": "VCZ6BxwBpfjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd"
      ],
      "metadata": {
        "id": "jogk5UrgeTkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a result, all pandas functions, methods, and created objects are proxies:"
      ],
      "metadata": {
        "id": "vxh70rpDph3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(pd.read_csv)"
      ],
      "metadata": {
        "id": "RYTCGl7spgjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Operations supported by cuDF will be **very** fast:"
      ],
      "metadata": {
        "id": "9-NvKu7XplmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df.count(axis=0)"
      ],
      "metadata": {
        "id": "MFvLJo4upnUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Operations not supported by cuDF will be slower, as they fall back to using Pandas (copying data between the CPU and GPU under the hood as needed). For example, cuDF does not currently support the `axis=` parameter to the `count` method. So this operation will run on the CPU and be noticeably slower than the previous one."
      ],
      "metadata": {
        "id": "Np6VP-wSpomO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df.count(axis=1) # This will use pandas, because cuDF doesn't support axis=1 for the .count() method"
      ],
      "metadata": {
        "id": "mThydJIYpuha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But the story doesn't end here. We often need to mix our own code with third-party libraries that other people have written. Many of these libraries accept pandas objects as inputs."
      ],
      "metadata": {
        "id": "tbDVvkP2pyra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using third-party libraries with cudf.pandas\n",
        "\n",
        "You can pass Pandas objects to third-party libraries when using `cudf.pandas`, just like you would when using regular Pandas.\n",
        "\n",
        "Below, we show an example of using [plotly-express](https://plotly.com/python/plotly-express/) to visualize the data we've been processing:"
      ],
      "metadata": {
        "id": "3yK3a-mIp0vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing which states have more pickup trucks relative to other vehicles?"
      ],
      "metadata": {
        "id": "H0QwPQcAp2RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "df = df.rename(columns={\n",
        "    \"Registration State\": \"reg_state\",\n",
        "    \"Vehicle Body Type\": \"vehicle_type\",\n",
        "})\n",
        "\n",
        "# vehicle counts per state:\n",
        "counts = df.groupby(\"reg_state\").size().sort_index()\n",
        "# vehicles with type \"PICK\" (Pickup Truck)\n",
        "pickup_counts = df.where(df[\"vehicle_type\"] == \"PICK\").groupby(\"reg_state\").size()\n",
        "# percentage of pickup trucks by state:\n",
        "pickup_frac = ((pickup_counts / counts) * 100).rename(\"% Pickup Trucks\")\n",
        "del pickup_frac[\"MB\"]  # (Manitoba is a huge outlier!)\n",
        "\n",
        "# plot the results:\n",
        "pickup_frac = pickup_frac.reset_index()\n",
        "px.choropleth(pickup_frac, locations=\"reg_state\", color=\"% Pickup Trucks\", locationmode=\"USA-states\", scope=\"usa\")"
      ],
      "metadata": {
        "id": "Ecs213eEqCd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beyond just passing data: **Accelerating** third-party code\n",
        "\n",
        "Being able to pass these proxy objects to libraries like Plotly is great, but the benefits don't end there.\n",
        "\n",
        "When you enable `cudf.pandas`, pandas operations running **inside the third-party library's functions** will also benefit from GPU acceleration where possible!\n",
        "\n",
        "Below, you can see an image illustrating how `cudf.pandas` can accelerate the pandas backend in Ibis, a library that provides a unified DataFrame API to various backends. We ran this example on a system with an NVIDIA H100 GPU and an Intel Xeon Platinum 8480CL CPU.\n",
        "\n",
        "\n",
        "By loading the `cudf.pandas` extension, pandas operations within Ibis can use the GPU with zero code change. It just works."
      ],
      "metadata": {
        "id": "9bgMrWs5qDG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![ibis](https://drive.google.com/uc?id=1uOJq2JtbgVb7tb8qw8a2gG3JRBo72t_H)"
      ],
      "metadata": {
        "id": "8JW2CQL6qEv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "With `cudf.pandas`, you can keep using pandas as your primary dataframe library. When things start to get a little slow, just load the `cudf.pandas` and run your existing code on a GPU!\n",
        "\n",
        "To learn more, we encourage you to visit [rapids.ai/cudf-pandas](https://rapids.ai/cudf-pandas)."
      ],
      "metadata": {
        "id": "pyVNtGUhtFs5"
      }
    }
  ]
}