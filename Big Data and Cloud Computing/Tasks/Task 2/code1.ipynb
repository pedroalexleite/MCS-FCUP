{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOmFuHnWwVWQ"
      },
      "source": [
        "# Programming GPUs in Python3 using PyCuda\n",
        "**[Big Data and Cloud Computing]**\n",
        "\n",
        "Most machines have available GPUs which we should take advantage of. GPUs can greatly accelerate execution if well programmed. Here we go through some examples.\n",
        "\n",
        "The example given in this notebook will only take advantage of a GPU if you have cuda installed and if you have a graphics card that is cuda-enabled. Please check [this wikipedia site](https://en.wikipedia.org/wiki/CUDA) or the official [nvidia site] (https://developer.nvidia.com/cuda-gpus) for more detail about compatibility. If you run this notebool in colab, it already provides you with a GPU in its runtime environment.\n",
        "\n",
        "__References__:\n",
        "\n",
        "- [Tutorial on PyCuda](https://documen.tician.de/pycuda/tutorial.html)\n",
        "- [CUDA programming guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrZK1VOHszlW",
        "outputId": "66f6d903-bc9b-49d4-8e96-de37209341c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pycuda"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2024.1.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2024.1.5-py2.py3-none-any.whl (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting appdirs>=1.4.0 (from pycuda)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting mako (from pycuda)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.5)\n",
            "Building wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2024.1-cp310-cp310-linux_x86_64.whl size=661205 sha256=b0dc11148f5d5794b3e5fe6a12a191db136dd6709a70a379f9c8e0de30cdf5d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/34/d2/9a349255a4eca3a486d82c79d21e138ce2ccd90f414d9d72b8\n",
            "Successfully built pycuda\n",
            "Installing collected packages: appdirs, pytools, mako, pycuda\n",
            "Successfully installed appdirs-1.4.4 mako-1.3.5 pycuda-2024.1 pytools-2024.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with a very simple code. We will create two vectors (A and B) and add them producing a third vector (C). The kernel code is reproduced below: (not executable yet)"
      ],
      "metadata": {
        "id": "YCbEwRHMYcYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// Kernel definition\n",
        "__global__ void VecAdd(float* A, float* B, float* C)\n",
        "{\n",
        "    int i = threadIdx.x;\n",
        "    C[i] = A[i] + B[i];\n",
        "}\n"
      ],
      "metadata": {
        "id": "bNk-5ae_Z77P",
        "outputId": "40567dde-0601-49e8-fc87-d5262c99634c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-4-ebfc4123d45d>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-ebfc4123d45d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    // Kernel definition\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make it executable by integrating it in a Python code. The array is created using `numpy`. We conveniently use numpy when working with GPUs, because GPUs can work very well on number crunching operations.\n",
        "\n",
        "(**NOTE**: if running in colab, you may need to configure your runtime to have a GPU. Click on the \"Runtime\" menu and then \"Change Runtime type\".)"
      ],
      "metadata": {
        "id": "AoBtj1J0aHTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################\n",
        "# GPU version\n",
        "############################\n",
        "\n",
        "# need to import modules\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "# end\n",
        "import numpy\n",
        "from time import time\n",
        "\n",
        "# begin timing\n",
        "start_time = time()\n",
        "########\n",
        "N = 16\n",
        "numpy.random.seed(123)\n",
        "a = numpy.random.rand(N)\n",
        "a = a.astype(numpy.float32)\n",
        "b = numpy.random.rand(N)\n",
        "b = b.astype(numpy.float32)\n",
        "c = numpy.zeros(shape=(1,N),dtype=numpy.float32)\n",
        "\n",
        "# need to allocate memory in the GPU to fit our array\n",
        "a_gpu = cuda.mem_alloc(a.nbytes)\n",
        "b_gpu = cuda.mem_alloc(b.nbytes)\n",
        "c_gpu = cuda.mem_alloc(c.nbytes)\n",
        "# a_tid = cuda.mem_alloc(b.nbytes)\n",
        "# a_bid = cuda.mem_alloc(c.nbytes)\n",
        "# need to copy our array to the GPU\n",
        "cuda.memcpy_htod(a_gpu, a)\n",
        "cuda.memcpy_htod(b_gpu, b)\n",
        "cuda.memcpy_htod(c_gpu, c)\n",
        "# cuda.memcpy_htod(a_tid, b)\n",
        "# cuda.memcpy_htod(a_bid, c)\n",
        "\n",
        "# here it is the kernel that will run the addition in the GPU\n",
        "mod = SourceModule(\"\"\"\n",
        "    __global__ void VecAdd(float* A, float* B, float* C)\n",
        "    {\n",
        "        int i = threadIdx.x;\n",
        "        C[i] = A[i] + B[i];\n",
        "    }\n",
        " \"\"\")\n",
        "func = mod.get_function(\"VecAdd\")\n",
        "# now, we define the shape\n",
        "func(a_gpu, b_gpu, c_gpu, block=(N,1,1))\n",
        "\n",
        "# create space in the host memory to receive results\n",
        "#c = numpy.empty_like(c)\n",
        "#c_tid_result = numpy.empty_like(c)\n",
        "\n",
        "# copy results from the GPU memory to the host memory\n",
        "cuda.memcpy_dtoh(c, c_gpu)\n",
        "\n",
        "# end timing\n",
        "print(round(time() - start_time,8), 'seconds')\n",
        "########\n",
        "\n",
        "# print(\"tid  bid\\n\")\n",
        "# for i in range(len(b)):\n",
        "#   s = \"\"\n",
        "#   for j in range(len(b[i])):\n",
        "#      s += str(b[i,j])+\"  \"+str(c[i,j])+\"  \"\n",
        "#   print(s)\n",
        "print(c)\n",
        "print(a)\n",
        "print(b)\n",
        "(a+b==c)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FvNPKMBLaRNs",
        "outputId": "22c55d95-7606-4d6c-96ae-33c6778629c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.98586082 seconds\n",
            "[[0.8789609  0.4615911  0.7584028  1.0831423  1.3538699  1.2725383\n",
            "  1.7052195  1.2958531  1.2033753  0.71507645 0.70496666 0.95731294\n",
            "  0.73228633 0.69065404 0.4901492  1.1716965 ]]\n",
            "[0.6964692  0.28613934 0.22685145 0.5513148  0.71946895 0.42310646\n",
            " 0.9807642  0.6848297  0.4809319  0.39211753 0.343178   0.7290497\n",
            " 0.43857226 0.0596779  0.39804426 0.7379954 ]\n",
            "[0.18249173 0.17545176 0.53155136 0.53182757 0.63440096 0.8494318\n",
            " 0.7244553  0.6110235  0.7224434  0.32295892 0.36178866 0.22826323\n",
            " 0.29371405 0.63097614 0.09210494 0.4337012 ]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGtCoIX5yc-b"
      },
      "source": [
        "Now, let's play with a bidimensional array with dimensions RxC. In this example, we create the matrix using `numpy` again. The function simply multiplies each cell of the matrix by 2. We create first a sequential version and next a cuda version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6CbREaruwRw",
        "outputId": "123d6bfa-fc69-43cf-b8d7-50b5c560ea80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "############################\n",
        "# Sequential version\n",
        "############################\n",
        "\n",
        "import numpy\n",
        "from time import time\n",
        "\n",
        "R = 16\n",
        "C = 16\n",
        "# begin timing\n",
        "start_time = time()\n",
        "########\n",
        "\n",
        "numpy.random.seed(123)\n",
        "a = numpy.random.randn(R,C)\n",
        "a_doubled = a*2\n",
        "\n",
        "# end timing\n",
        "print(round(time() - start_time,8), 'seconds')\n",
        "########\n",
        "\n",
        "print(a_doubled)\n",
        "print(a)\n",
        "a*2==a_doubled"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00130296 seconds\n",
            "[[-2.17126121e+00  1.99469089e+00  5.65956996e-01 -3.01258943e+00\n",
            "  -1.15720050e+00  3.30287307e+00 -4.85335849e+00 -8.57825258e-01\n",
            "   2.53187252e+00 -1.73348080e+00 -1.35777230e+00 -1.89417938e-01\n",
            "   2.98277925e+00 -1.27780399e+00 -8.87963919e-01 -8.68702551e-01]\n",
            " [ 4.41186017e+00  4.37357218e+00  2.00810780e+00  7.72372798e-01\n",
            "   1.47473715e+00  2.98146406e+00 -1.87166774e+00  2.35165809e+00\n",
            "  -2.50776134e+00 -1.27550300e+00  1.81421039e+00 -2.85736140e+00\n",
            "  -2.80137440e-01 -1.72350979e+00 -5.11238741e-01 -5.59717821e+00]\n",
            " [-3.54306621e+00 -1.39975447e+00  1.85492486e+00 -3.47271366e-01\n",
            "   5.69183179e-03  1.37644542e+00 -1.75907269e+00  5.67254648e-01\n",
            "  -1.61073304e+00 -3.45533899e+00 -7.81799588e-01  1.14761172e+00\n",
            "   6.77178102e-01 -2.36609890e-02  4.78473053e+00  8.25824321e-01]\n",
            " [ 1.95747201e+00  4.47628668e+00 -2.58817065e+00 -2.07757642e+00\n",
            "   3.48742445e+00 -1.59612547e+00  5.93664606e-02  2.13863194e+00\n",
            "   1.78141278e+00  3.50977236e+00  2.99128827e+00  2.13878534e+00\n",
            "  -1.54541743e+00  1.58972534e+00  6.28543989e-01 -2.65253092e+00]\n",
            " [ 2.83459809e+00  1.61447307e+00  9.09801613e-02 -4.66184122e-01\n",
            "  -2.39660229e+00  3.99048147e-01  9.36878239e-01 -1.66230997e+00\n",
            "   2.32440810e+00 -2.19440609e+00 -4.24620070e+00  2.07945418e+00\n",
            "  -8.06732076e-01 -2.52059171e-01 -1.67503345e+00 -3.21192552e+00]\n",
            " [ 2.51047475e+00 -1.37773797e+00  3.32190498e+00  1.61461637e+00\n",
            "  -6.29516293e-01 -2.17180480e+00 -1.46492397e+00 -2.42504626e+00\n",
            "   4.17422672e+00  3.28882460e-01  2.30041109e+00 -2.53470410e+00\n",
            "   3.62070259e-01  2.35572388e+00 -6.70021524e-01  2.06222892e+00]\n",
            " [-2.16913582e+00 -2.72694309e+00  7.58801224e-01 -7.58352869e-01\n",
            "   1.28410938e+00 -3.95577586e+00  1.42452927e+00  5.19660785e+00\n",
            "  -4.92519629e-02  6.82842578e-02  3.59098969e-01 -3.72395142e+00\n",
            "   8.52293279e-01 -3.21081949e+00 -8.55359196e-01  2.48573910e+00]\n",
            " [-1.47043391e+00  1.00249798e+00  2.02547811e+00  5.57481712e-01\n",
            "  -2.74189694e+00 -6.64950551e-01  3.91882268e+00 -4.05009153e+00\n",
            "  -5.51572028e-01 -1.10421614e+00  2.41494726e-01  1.49643123e+00\n",
            "   3.21738194e+00 -5.40464784e-01  1.62468266e+00  9.99480290e-01]\n",
            " [ 9.48694596e-01 -1.12784786e+00 -1.99464294e+00 -2.20008623e+00\n",
            "  -1.51287442e+00  6.43373152e-01  1.52189879e+00  6.46937696e-01\n",
            "  -1.09791019e+00  3.61194022e+00  3.03773125e+00 -7.08000226e-01\n",
            "  -1.64686281e+00  2.60429908e-01  2.53459729e+00  6.65529955e-01]\n",
            " [ 1.11309741e+00 -4.24160245e-01  9.12541790e-01  3.08908890e+00\n",
            "  -4.79337563e-01  2.86615465e-01  5.07632953e-01  5.67450712e-01\n",
            "  -2.82377775e+00 -3.75373731e+00 -2.03931014e+00  3.35884591e-01\n",
            "   1.10771233e+00 -1.06134912e+00  2.75451497e+00 -2.86351949e-01]\n",
            " [ 4.06319965e-02 -3.87927741e-01  2.68053585e-01  1.40894815e+00\n",
            "   1.33130688e+00 -1.79684588e+00  3.04732755e+00 -2.19005291e+00\n",
            "   1.58454028e-01 -5.48793147e-01 -2.09798335e+00 -1.50241176e-01\n",
            "  -1.48162755e+00  1.45814487e-01  8.06171922e-01  2.94385874e+00]\n",
            " [ 6.14768437e-01 -1.22245068e+00 -7.83239621e-01  2.79956212e-01\n",
            "   1.86921659e-01  2.91917854e+00  2.79070586e+00 -7.17871852e-01\n",
            "  -1.09728426e+00 -5.11410921e+00 -1.09784083e+00 -1.95611541e+00\n",
            "  -7.09648916e-01  7.83168485e-01  3.54384658e-01 -5.99360141e-02]\n",
            " [ 3.99164222e-01 -2.52235546e-01  3.94037865e-01 -6.46211002e+00\n",
            "  -5.38586980e-01 -2.21701442e-01 -6.82523432e-01 -4.35892524e-01\n",
            "   1.40662024e+00 -1.19621066e+00  4.40140420e+00  1.37659386e+00\n",
            "  -1.26145018e-02 -4.13324606e-01 -1.73044573e-01 -1.83061414e+00]\n",
            " [-1.90405079e-01  5.57367033e-01  1.15908323e+00  1.15937956e+00\n",
            "  -5.49755093e-01 -2.83216451e+00 -1.33820525e+00  3.22438609e+00\n",
            "   1.79211663e+00  7.39239173e-01 -1.52258849e+00  7.29030995e-03\n",
            "  -2.51133737e+00 -1.10387375e+00 -4.90406683e-01 -7.23279864e-01]\n",
            " [ 1.91320386e+00 -2.83745183e+00 -1.73086454e+00 -2.74937594e+00\n",
            "  -2.47470643e+00  2.48111793e-01 -3.20088107e+00  1.50773756e+00\n",
            "  -4.93631554e-01  1.37576665e-01  6.45153476e-01 -8.68333041e-01\n",
            "   2.06495943e+00 -3.88685453e-01  1.18814051e+00 -3.98224766e-01]\n",
            " [ 5.81748775e-01  5.59325340e-01  4.99939879e-01 -1.94861570e+00\n",
            "   8.71751541e-01 -6.37913978e-01  1.26097606e+00 -4.30498689e+00\n",
            "  -2.93023244e+00  7.26891124e-01  3.72585517e+00  1.67011749e+00\n",
            "  -1.36490187e+00 -3.38410483e+00  1.48537244e+00 -1.61166444e-01]]\n",
            "[[-1.08563060e+00  9.97345447e-01  2.82978498e-01 -1.50629471e+00\n",
            "  -5.78600252e-01  1.65143654e+00 -2.42667924e+00 -4.28912629e-01\n",
            "   1.26593626e+00 -8.66740402e-01 -6.78886152e-01 -9.47089689e-02\n",
            "   1.49138963e+00 -6.38901997e-01 -4.43981960e-01 -4.34351276e-01]\n",
            " [ 2.20593008e+00  2.18678609e+00  1.00405390e+00  3.86186399e-01\n",
            "   7.37368576e-01  1.49073203e+00 -9.35833868e-01  1.17582904e+00\n",
            "  -1.25388067e+00 -6.37751502e-01  9.07105196e-01 -1.42868070e+00\n",
            "  -1.40068720e-01 -8.61754896e-01 -2.55619371e-01 -2.79858911e+00]\n",
            " [-1.77153310e+00 -6.99877235e-01  9.27462432e-01 -1.73635683e-01\n",
            "   2.84591590e-03  6.88222711e-01 -8.79536343e-01  2.83627324e-01\n",
            "  -8.05366518e-01 -1.72766949e+00 -3.90899794e-01  5.73805862e-01\n",
            "   3.38589051e-01 -1.18304945e-02  2.39236527e+00  4.12912160e-01]\n",
            " [ 9.78736006e-01  2.23814334e+00 -1.29408532e+00 -1.03878821e+00\n",
            "   1.74371223e+00 -7.98062735e-01  2.96832303e-02  1.06931597e+00\n",
            "   8.90706391e-01  1.75488618e+00  1.49564414e+00  1.06939267e+00\n",
            "  -7.72708714e-01  7.94862668e-01  3.14271995e-01 -1.32626546e+00]\n",
            " [ 1.41729905e+00  8.07236535e-01  4.54900806e-02 -2.33092061e-01\n",
            "  -1.19830114e+00  1.99524074e-01  4.68439119e-01 -8.31154984e-01\n",
            "   1.16220405e+00 -1.09720305e+00 -2.12310035e+00  1.03972709e+00\n",
            "  -4.03366038e-01 -1.26029585e-01 -8.37516723e-01 -1.60596276e+00]\n",
            " [ 1.25523737e+00 -6.88868984e-01  1.66095249e+00  8.07308186e-01\n",
            "  -3.14758147e-01 -1.08590240e+00 -7.32461987e-01 -1.21252313e+00\n",
            "   2.08711336e+00  1.64441230e-01  1.15020554e+00 -1.26735205e+00\n",
            "   1.81035130e-01  1.17786194e+00 -3.35010762e-01  1.03111446e+00]\n",
            " [-1.08456791e+00 -1.36347154e+00  3.79400612e-01 -3.79176435e-01\n",
            "   6.42054689e-01 -1.97788793e+00  7.12264635e-01  2.59830393e+00\n",
            "  -2.46259814e-02  3.41421289e-02  1.79549485e-01 -1.86197571e+00\n",
            "   4.26146640e-01 -1.60540974e+00 -4.27679598e-01  1.24286955e+00]\n",
            " [-7.35216956e-01  5.01248990e-01  1.01273905e+00  2.78740856e-01\n",
            "  -1.37094847e+00 -3.32475275e-01  1.95941134e+00 -2.02504576e+00\n",
            "  -2.75786014e-01 -5.52108071e-01  1.20747363e-01  7.48215617e-01\n",
            "   1.60869097e+00 -2.70232392e-01  8.12341330e-01  4.99740145e-01]\n",
            " [ 4.74347298e-01 -5.63923932e-01 -9.97321469e-01 -1.10004311e+00\n",
            "  -7.56437209e-01  3.21686576e-01  7.60949393e-01  3.23468848e-01\n",
            "  -5.48955096e-01  1.80597011e+00  1.51886562e+00 -3.54000113e-01\n",
            "  -8.23431406e-01  1.30214954e-01  1.26729865e+00  3.32764977e-01]\n",
            " [ 5.56548705e-01 -2.12080122e-01  4.56270895e-01  1.54454445e+00\n",
            "  -2.39668781e-01  1.43307733e-01  2.53816477e-01  2.83725356e-01\n",
            "  -1.41188888e+00 -1.87686866e+00 -1.01965507e+00  1.67942295e-01\n",
            "   5.53856166e-01 -5.30674560e-01  1.37725748e+00 -1.43175974e-01]\n",
            " [ 2.03159982e-02 -1.93963871e-01  1.34026793e-01  7.04474074e-01\n",
            "   6.65653438e-01 -8.98422941e-01  1.52366378e+00 -1.09502646e+00\n",
            "   7.92270141e-02 -2.74396574e-01 -1.04899168e+00 -7.51205880e-02\n",
            "  -7.40813773e-01  7.29072433e-02  4.03085961e-01  1.47192937e+00]\n",
            " [ 3.07384219e-01 -6.11225340e-01 -3.91619811e-01  1.39978106e-01\n",
            "   9.34608295e-02  1.45958927e+00  1.39535293e+00 -3.58935926e-01\n",
            "  -5.48642128e-01 -2.55705460e+00 -5.48920413e-01 -9.78057706e-01\n",
            "  -3.54824458e-01  3.91584242e-01  1.77192329e-01 -2.99680070e-02]\n",
            " [ 1.99582111e-01 -1.26117773e-01  1.97018933e-01 -3.23105501e+00\n",
            "  -2.69293490e-01 -1.10850721e-01 -3.41261716e-01 -2.17946262e-01\n",
            "   7.03310118e-01 -5.98105331e-01  2.20070210e+00  6.88296930e-01\n",
            "  -6.30725091e-03 -2.06662303e-01 -8.65222864e-02 -9.15307070e-01]\n",
            " [-9.52025393e-02  2.78683517e-01  5.79541616e-01  5.79689779e-01\n",
            "  -2.74877547e-01 -1.41608225e+00 -6.69102626e-01  1.61219304e+00\n",
            "   8.96058313e-01  3.69619586e-01 -7.61294245e-01  3.64515497e-03\n",
            "  -1.25566869e+00 -5.51936876e-01 -2.45203342e-01 -3.61639932e-01]\n",
            " [ 9.56601931e-01 -1.41872591e+00 -8.65432272e-01 -1.37468797e+00\n",
            "  -1.23735321e+00  1.24055896e-01 -1.60044053e+00  7.53868779e-01\n",
            "  -2.46815777e-01  6.87883325e-02  3.22576738e-01 -4.34166520e-01\n",
            "   1.03247972e+00 -1.94342727e-01  5.94070255e-01 -1.99112383e-01]\n",
            " [ 2.90874387e-01  2.79662670e-01  2.49969939e-01 -9.74307850e-01\n",
            "   4.35875771e-01 -3.18956989e-01  6.30488030e-01 -2.15249344e+00\n",
            "  -1.46511622e+00  3.63445562e-01  1.86292759e+00  8.35058747e-01\n",
            "  -6.82450934e-01 -1.69205242e+00  7.42686221e-01 -8.05832220e-02]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True],\n",
              "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################\n",
        "# Sequential version\n",
        "############################\n",
        "\n",
        "import numpy\n",
        "from time import time\n",
        "\n",
        "R = 10240\n",
        "C = 10240\n",
        "# begin timing\n",
        "start_time = time()\n",
        "########\n",
        "\n",
        "numpy.random.seed(123)\n",
        "a = numpy.random.randn(R,C)\n",
        "a_doubled = a*2\n",
        "\n",
        "# end timing\n",
        "print(round(time() - start_time,8), 'seconds')\n",
        "########\n",
        "\n",
        "print(a_doubled)\n",
        "print(a)\n",
        "a*2==a_doubled"
      ],
      "metadata": {
        "id": "eYYpJuaZZMxS",
        "outputId": "2a35fdc2-c50e-4f1d-e024-d0621ac84d21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.28372169 seconds\n",
            "[[-2.17126121  1.99469089  0.565957   ...  1.72446443  3.60881851\n",
            "  -2.88587644]\n",
            " [-0.47114058 -1.34853623 -1.65642254 ... -3.33163526  0.38023076\n",
            "   0.42766382]\n",
            " [-0.95525402  1.9486294   1.80930121 ... -2.06418585 -0.95221937\n",
            "  -0.94392159]\n",
            " ...\n",
            " [-1.29066852  3.17536121  3.46727044 ...  1.32611824 -2.34483286\n",
            "   0.11799299]\n",
            " [-1.82653166 -2.42549176  0.23146129 ...  2.57736052 -1.27727782\n",
            "   1.14314436]\n",
            " [-2.74400789  0.57926116 -2.67865638 ...  0.09464489 -0.57723126\n",
            "   0.28486693]]\n",
            "[[-1.0856306   0.99734545  0.2829785  ...  0.86223221  1.80440926\n",
            "  -1.44293822]\n",
            " [-0.23557029 -0.67426812 -0.82821127 ... -1.66581763  0.19011538\n",
            "   0.21383191]\n",
            " [-0.47762701  0.9743147   0.9046506  ... -1.03209292 -0.47610968\n",
            "  -0.4719608 ]\n",
            " ...\n",
            " [-0.64533426  1.5876806   1.73363522 ...  0.66305912 -1.17241643\n",
            "   0.05899649]\n",
            " [-0.91326583 -1.21274588  0.11573065 ...  1.28868026 -0.63863891\n",
            "   0.57157218]\n",
            " [-1.37200394  0.28963058 -1.33932819 ...  0.04732245 -0.28861563\n",
            "   0.14243346]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True,  True,  True, ...,  True,  True,  True],\n",
              "       [ True,  True,  True, ...,  True,  True,  True],\n",
              "       [ True,  True,  True, ...,  True,  True,  True],\n",
              "       ...,\n",
              "       [ True,  True,  True, ...,  True,  True,  True],\n",
              "       [ True,  True,  True, ...,  True,  True,  True],\n",
              "       [ True,  True,  True, ...,  True,  True,  True]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNQsFYpcy3QM"
      },
      "source": [
        "Now, let's \"decorate\" this code to use a GPU to compute all multiplications in parallel. In this example, we use auxiliary arrays that will contain the thread IDs and block IDs, so that we can inspect the number of threads working and what each one is doing. Thread IDs Block IDs are stored in the GPU, in vectors called a_tid and a_bid, respectively. In the host they are called b and c. Our matrix is called `a` in the host and `a_gpu` in the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAcygnW-41tU",
        "outputId": "f276867d-c969-413b-bb95-586530bb2277",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "############################\n",
        "# GPU version\n",
        "############################\n",
        "\n",
        "# need to import modules\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "# end\n",
        "import numpy\n",
        "from time import time\n",
        "\n",
        "# begin timing\n",
        "start_time = time()\n",
        "########\n",
        "R = 16\n",
        "C = 16\n",
        "numpy.random.seed(123)\n",
        "a = numpy.random.randn(R,C)\n",
        "a = a.astype(numpy.float32)\n",
        "b = numpy.zeros(shape=(R,C),dtype=numpy.uint32)\n",
        "c = numpy.zeros(shape=(R,C),dtype=numpy.uint32)\n",
        "\n",
        "# need to allocate memory in the GPU to fit our array\n",
        "a_gpu = cuda.mem_alloc(a.nbytes)\n",
        "a_tid = cuda.mem_alloc(b.nbytes)\n",
        "a_bid = cuda.mem_alloc(c.nbytes)\n",
        "# need to copy our array to the GPU\n",
        "cuda.memcpy_htod(a_gpu, a)\n",
        "cuda.memcpy_htod(a_tid, b)\n",
        "cuda.memcpy_htod(a_bid, c)\n",
        "\n",
        "# here it is the kernel that will run the multiplication in the GPU\n",
        "mod = SourceModule(\"\"\"\n",
        "  __global__ void doublify(float *a, uint *a_tid, uint *a_bid)\n",
        "  {\n",
        "    int idx = threadIdx.x + threadIdx.y*16;\n",
        "    a[idx] *= 2;\n",
        "    a_tid[idx] = threadIdx.x + blockDim.x * threadIdx.y;\n",
        "    a_bid[idx] = blockDim.x * blockDim.y;\n",
        "\n",
        "  }\n",
        "  \"\"\")\n",
        "func = mod.get_function(\"doublify\")\n",
        "# now, we define the shape\n",
        "func(a_gpu, a_tid, a_bid, block=(1,R,C))\n",
        "\n",
        "# create space in the host memory to receive results\n",
        "a_doubled = numpy.empty_like(a)\n",
        "a_tid_result = numpy.empty_like(b)\n",
        "\n",
        "# copy results from the GPU memory to the host memory\n",
        "cuda.memcpy_dtoh(a_doubled, a_gpu)\n",
        "cuda.memcpy_dtoh(b, a_tid)\n",
        "cuda.memcpy_dtoh(c, a_bid)\n",
        "\n",
        "\n",
        "# end timing\n",
        "print(round(time() - start_time,8), 'seconds')\n",
        "########\n",
        "\n",
        "print(\"tid  bid\\n\")\n",
        "for i in range(len(b)):\n",
        "  s = \"\"\n",
        "  for j in range(len(b[i])):\n",
        "     s += str(b[i,j])+\"  \"+str(c[i,j])+\"  \"\n",
        "  print(s)\n",
        "print(a_doubled)\n",
        "print(a)\n",
        "(a*2==a_doubled)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: device_allocation in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.64592648 seconds\n",
            "tid  bid\n",
            "\n",
            "0  16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
            "1  16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
            "2  16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
            "3  16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
            "4  16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
            "5  16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
            "6  16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
            "7  16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
            "8  16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
            "9  16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
            "10  16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
            "11  16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
            "12  16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
            "13  16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
            "14  16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
            "15  16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  \n",
            "[[-2.17126131e+00  9.97345448e-01  2.82978505e-01 -1.50629473e+00\n",
            "  -5.78600228e-01  1.65143657e+00 -2.42667913e+00 -4.28912640e-01\n",
            "   1.26593626e+00 -8.66740406e-01 -6.78886175e-01 -9.47089717e-02\n",
            "   1.49138963e+00 -6.38902009e-01 -4.43981946e-01 -4.34351265e-01]\n",
            " [ 4.41185999e+00  2.18678617e+00  1.00405395e+00  3.86186391e-01\n",
            "   7.37368584e-01  1.49073207e+00 -9.35833871e-01  1.17582905e+00\n",
            "  -1.25388062e+00 -6.37751520e-01  9.07105207e-01 -1.42868066e+00\n",
            "  -1.40068725e-01 -8.61754894e-01 -2.55619377e-01 -2.79858899e+00]\n",
            " [-3.54306626e+00 -6.99877262e-01  9.27462459e-01 -1.73635677e-01\n",
            "   2.84591597e-03  6.88222706e-01 -8.79536331e-01  2.83627331e-01\n",
            "  -8.05366516e-01 -1.72766948e+00 -3.90899807e-01  5.73805869e-01\n",
            "   3.38589042e-01 -1.18304947e-02  2.39236522e+00  4.12912160e-01]\n",
            " [ 1.95747197e+00  2.23814344e+00 -1.29408526e+00 -1.03878820e+00\n",
            "   1.74371219e+00 -7.98062742e-01  2.96832304e-02  1.06931591e+00\n",
            "   8.90706420e-01  1.75488615e+00  1.49564409e+00  1.06939268e+00\n",
            "  -7.72708714e-01  7.94862688e-01  3.14271986e-01 -1.32626545e+00]\n",
            " [ 2.83459806e+00  8.07236552e-01  4.54900824e-02 -2.33092055e-01\n",
            "  -1.19830120e+00  1.99524075e-01  4.68439132e-01 -8.31155002e-01\n",
            "   1.16220403e+00 -1.09720302e+00 -2.12310028e+00  1.03972709e+00\n",
            "  -4.03366029e-01 -1.26029581e-01 -8.37516725e-01 -1.60596275e+00]\n",
            " [ 2.51047468e+00 -6.88868999e-01  1.66095245e+00  8.07308197e-01\n",
            "  -3.14758152e-01 -1.08590245e+00 -7.32461989e-01 -1.21252310e+00\n",
            "   2.08711338e+00  1.64441228e-01  1.15020549e+00 -1.26735210e+00\n",
            "   1.81035131e-01  1.17786193e+00 -3.35010767e-01  1.03111446e+00]\n",
            " [-2.16913581e+00 -1.36347151e+00  3.79400611e-01 -3.79176438e-01\n",
            "   6.42054677e-01 -1.97788799e+00  7.12264657e-01  2.59830403e+00\n",
            "  -2.46259812e-02  3.41421291e-02  1.79549485e-01 -1.86197567e+00\n",
            "   4.26146626e-01 -1.60540974e+00 -4.27679598e-01  1.24286950e+00]\n",
            " [-1.47043395e+00  5.01249015e-01  1.01273906e+00  2.78740853e-01\n",
            "  -1.37094843e+00 -3.32475275e-01  1.95941138e+00 -2.02504587e+00\n",
            "  -2.75786012e-01 -5.52108049e-01  1.20747365e-01  7.48215616e-01\n",
            "   1.60869098e+00 -2.70232379e-01  8.12341332e-01  4.99740154e-01]\n",
            " [ 9.48694587e-01 -5.63923955e-01 -9.97321486e-01 -1.10004306e+00\n",
            "  -7.56437182e-01  3.21686566e-01  7.60949373e-01  3.23468834e-01\n",
            "  -5.48955083e-01  1.80597007e+00  1.51886559e+00 -3.54000121e-01\n",
            "  -8.23431432e-01  1.30214959e-01  1.26729870e+00  3.32764983e-01]\n",
            " [ 1.11309743e+00 -2.12080121e-01  4.56270903e-01  1.54454446e+00\n",
            "  -2.39668787e-01  1.43307731e-01  2.53816485e-01  2.83725351e-01\n",
            "  -1.41188884e+00 -1.87686861e+00 -1.01965511e+00  1.67942300e-01\n",
            "   5.53856194e-01 -5.30674577e-01  1.37725747e+00 -1.43175974e-01]\n",
            " [ 4.06319983e-02 -1.93963870e-01  1.34026796e-01  7.04474092e-01\n",
            "   6.65653467e-01 -8.98422956e-01  1.52366376e+00 -1.09502649e+00\n",
            "   7.92270154e-02 -2.74396569e-01 -1.04899168e+00 -7.51205906e-02\n",
            "  -7.40813792e-01  7.29072466e-02  4.03085947e-01  1.47192931e+00]\n",
            " [ 6.14768445e-01 -6.11225367e-01 -3.91619802e-01  1.39978111e-01\n",
            "   9.34608281e-02  1.45958924e+00  1.39535296e+00 -3.58935922e-01\n",
            "  -5.48642099e-01 -2.55705452e+00 -5.48920393e-01 -9.78057683e-01\n",
            "  -3.54824454e-01  3.91584247e-01  1.77192330e-01 -2.99680065e-02]\n",
            " [ 3.99164230e-01 -1.26117766e-01  1.97018936e-01 -3.23105502e+00\n",
            "  -2.69293487e-01 -1.10850722e-01 -3.41261715e-01 -2.17946261e-01\n",
            "   7.03310132e-01 -5.98105311e-01  2.20070219e+00  6.88296914e-01\n",
            "  -6.30725082e-03 -2.06662297e-01 -8.65222886e-02 -9.15307045e-01]\n",
            " [-1.90405086e-01  2.78683513e-01  5.79541624e-01  5.79689801e-01\n",
            "  -2.74877548e-01 -1.41608226e+00 -6.69102609e-01  1.61219299e+00\n",
            "   8.96058321e-01  3.69619578e-01 -7.61294246e-01  3.64515488e-03\n",
            "  -1.25566864e+00 -5.51936865e-01 -2.45203346e-01 -3.61639947e-01]\n",
            " [ 1.91320384e+00 -1.41872597e+00 -8.65432262e-01 -1.37468803e+00\n",
            "  -1.23735321e+00  1.24055900e-01 -1.60044050e+00  7.53868759e-01\n",
            "  -2.46815771e-01  6.87883347e-02  3.22576731e-01 -4.34166521e-01\n",
            "   1.03247976e+00 -1.94342732e-01  5.94070256e-01 -1.99112386e-01]\n",
            " [ 5.81748784e-01  2.79662669e-01  2.49969944e-01 -9.74307835e-01\n",
            "   4.35875773e-01 -3.18957001e-01  6.30488038e-01 -2.15249348e+00\n",
            "  -1.46511626e+00  3.63445550e-01  1.86292756e+00  8.35058749e-01\n",
            "  -6.82450950e-01 -1.69205236e+00  7.42686212e-01 -8.05832222e-02]]\n",
            "[[-1.08563066e+00  9.97345448e-01  2.82978505e-01 -1.50629473e+00\n",
            "  -5.78600228e-01  1.65143657e+00 -2.42667913e+00 -4.28912640e-01\n",
            "   1.26593626e+00 -8.66740406e-01 -6.78886175e-01 -9.47089717e-02\n",
            "   1.49138963e+00 -6.38902009e-01 -4.43981946e-01 -4.34351265e-01]\n",
            " [ 2.20592999e+00  2.18678617e+00  1.00405395e+00  3.86186391e-01\n",
            "   7.37368584e-01  1.49073207e+00 -9.35833871e-01  1.17582905e+00\n",
            "  -1.25388062e+00 -6.37751520e-01  9.07105207e-01 -1.42868066e+00\n",
            "  -1.40068725e-01 -8.61754894e-01 -2.55619377e-01 -2.79858899e+00]\n",
            " [-1.77153313e+00 -6.99877262e-01  9.27462459e-01 -1.73635677e-01\n",
            "   2.84591597e-03  6.88222706e-01 -8.79536331e-01  2.83627331e-01\n",
            "  -8.05366516e-01 -1.72766948e+00 -3.90899807e-01  5.73805869e-01\n",
            "   3.38589042e-01 -1.18304947e-02  2.39236522e+00  4.12912160e-01]\n",
            " [ 9.78735983e-01  2.23814344e+00 -1.29408526e+00 -1.03878820e+00\n",
            "   1.74371219e+00 -7.98062742e-01  2.96832304e-02  1.06931591e+00\n",
            "   8.90706420e-01  1.75488615e+00  1.49564409e+00  1.06939268e+00\n",
            "  -7.72708714e-01  7.94862688e-01  3.14271986e-01 -1.32626545e+00]\n",
            " [ 1.41729903e+00  8.07236552e-01  4.54900824e-02 -2.33092055e-01\n",
            "  -1.19830120e+00  1.99524075e-01  4.68439132e-01 -8.31155002e-01\n",
            "   1.16220403e+00 -1.09720302e+00 -2.12310028e+00  1.03972709e+00\n",
            "  -4.03366029e-01 -1.26029581e-01 -8.37516725e-01 -1.60596275e+00]\n",
            " [ 1.25523734e+00 -6.88868999e-01  1.66095245e+00  8.07308197e-01\n",
            "  -3.14758152e-01 -1.08590245e+00 -7.32461989e-01 -1.21252310e+00\n",
            "   2.08711338e+00  1.64441228e-01  1.15020549e+00 -1.26735210e+00\n",
            "   1.81035131e-01  1.17786193e+00 -3.35010767e-01  1.03111446e+00]\n",
            " [-1.08456790e+00 -1.36347151e+00  3.79400611e-01 -3.79176438e-01\n",
            "   6.42054677e-01 -1.97788799e+00  7.12264657e-01  2.59830403e+00\n",
            "  -2.46259812e-02  3.41421291e-02  1.79549485e-01 -1.86197567e+00\n",
            "   4.26146626e-01 -1.60540974e+00 -4.27679598e-01  1.24286950e+00]\n",
            " [-7.35216975e-01  5.01249015e-01  1.01273906e+00  2.78740853e-01\n",
            "  -1.37094843e+00 -3.32475275e-01  1.95941138e+00 -2.02504587e+00\n",
            "  -2.75786012e-01 -5.52108049e-01  1.20747365e-01  7.48215616e-01\n",
            "   1.60869098e+00 -2.70232379e-01  8.12341332e-01  4.99740154e-01]\n",
            " [ 4.74347293e-01 -5.63923955e-01 -9.97321486e-01 -1.10004306e+00\n",
            "  -7.56437182e-01  3.21686566e-01  7.60949373e-01  3.23468834e-01\n",
            "  -5.48955083e-01  1.80597007e+00  1.51886559e+00 -3.54000121e-01\n",
            "  -8.23431432e-01  1.30214959e-01  1.26729870e+00  3.32764983e-01]\n",
            " [ 5.56548715e-01 -2.12080121e-01  4.56270903e-01  1.54454446e+00\n",
            "  -2.39668787e-01  1.43307731e-01  2.53816485e-01  2.83725351e-01\n",
            "  -1.41188884e+00 -1.87686861e+00 -1.01965511e+00  1.67942300e-01\n",
            "   5.53856194e-01 -5.30674577e-01  1.37725747e+00 -1.43175974e-01]\n",
            " [ 2.03159992e-02 -1.93963870e-01  1.34026796e-01  7.04474092e-01\n",
            "   6.65653467e-01 -8.98422956e-01  1.52366376e+00 -1.09502649e+00\n",
            "   7.92270154e-02 -2.74396569e-01 -1.04899168e+00 -7.51205906e-02\n",
            "  -7.40813792e-01  7.29072466e-02  4.03085947e-01  1.47192931e+00]\n",
            " [ 3.07384223e-01 -6.11225367e-01 -3.91619802e-01  1.39978111e-01\n",
            "   9.34608281e-02  1.45958924e+00  1.39535296e+00 -3.58935922e-01\n",
            "  -5.48642099e-01 -2.55705452e+00 -5.48920393e-01 -9.78057683e-01\n",
            "  -3.54824454e-01  3.91584247e-01  1.77192330e-01 -2.99680065e-02]\n",
            " [ 1.99582115e-01 -1.26117766e-01  1.97018936e-01 -3.23105502e+00\n",
            "  -2.69293487e-01 -1.10850722e-01 -3.41261715e-01 -2.17946261e-01\n",
            "   7.03310132e-01 -5.98105311e-01  2.20070219e+00  6.88296914e-01\n",
            "  -6.30725082e-03 -2.06662297e-01 -8.65222886e-02 -9.15307045e-01]\n",
            " [-9.52025428e-02  2.78683513e-01  5.79541624e-01  5.79689801e-01\n",
            "  -2.74877548e-01 -1.41608226e+00 -6.69102609e-01  1.61219299e+00\n",
            "   8.96058321e-01  3.69619578e-01 -7.61294246e-01  3.64515488e-03\n",
            "  -1.25566864e+00 -5.51936865e-01 -2.45203346e-01 -3.61639947e-01]\n",
            " [ 9.56601918e-01 -1.41872597e+00 -8.65432262e-01 -1.37468803e+00\n",
            "  -1.23735321e+00  1.24055900e-01 -1.60044050e+00  7.53868759e-01\n",
            "  -2.46815771e-01  6.87883347e-02  3.22576731e-01 -4.34166521e-01\n",
            "   1.03247976e+00 -1.94342732e-01  5.94070256e-01 -1.99112386e-01]\n",
            " [ 2.90874392e-01  2.79662669e-01  2.49969944e-01 -9.74307835e-01\n",
            "   4.35875773e-01 -3.18957001e-01  6.30488038e-01 -2.15249348e+00\n",
            "  -1.46511626e+00  3.63445550e-01  1.86292756e+00  8.35058749e-01\n",
            "  -6.82450950e-01 -1.69205236e+00  7.42686212e-01 -8.05832222e-02]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False],\n",
              "       [ True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False],\n",
              "       [ True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False],\n",
              "       [ True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False],\n",
              "       [ True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False],\n",
              "       [ True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False],\n",
              "       [ True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False],\n",
              "       [ True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False],\n",
              "       [ True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False],\n",
              "       [ True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False],\n",
              "       [ True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False],\n",
              "       [ True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False],\n",
              "       [ True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False],\n",
              "       [ True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False],\n",
              "       [ True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False],\n",
              "       [ True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################\n",
        "# GPU version\n",
        "############################\n",
        "\n",
        "# need to import modules\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "import numpy\n",
        "from time import time\n",
        "\n",
        "# begin timing\n",
        "start_time = time()\n",
        "\n",
        "########\n",
        "R = 10240\n",
        "C = 10240\n",
        "numpy.random.seed(123)\n",
        "a = numpy.random.randn(R, C)\n",
        "a = a.astype(numpy.float32)\n",
        "b = numpy.zeros(shape=(R, C), dtype=numpy.uint32)\n",
        "c = numpy.zeros(shape=(R, C), dtype=numpy.uint32)\n",
        "\n",
        "# need to allocate memory in the GPU to fit our array\n",
        "a_gpu = cuda.mem_alloc(a.nbytes)\n",
        "a_tid = cuda.mem_alloc(b.nbytes)\n",
        "a_bid = cuda.mem_alloc(c.nbytes)\n",
        "\n",
        "# need to copy our array to the GPU\n",
        "cuda.memcpy_htod(a_gpu, a)\n",
        "cuda.memcpy_htod(a_tid, b)\n",
        "cuda.memcpy_htod(a_bid, c)\n",
        "\n",
        "# define the kernel that will run the multiplication in the GPU\n",
        "mod = SourceModule(\"\"\"\n",
        "  __global__ void doublify(float *a, uint *a_tid, uint *a_bid, int R, int C)\n",
        "  {\n",
        "    int idx_x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int idx_y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int idx = idx_y * C + idx_x;\n",
        "\n",
        "    if (idx_x < C && idx_y < R) {\n",
        "        a[idx] *= 2;\n",
        "        a_tid[idx] = idx_x + blockDim.x * idx_y;\n",
        "        a_bid[idx] = blockDim.x * blockDim.y;\n",
        "    }\n",
        "  }\n",
        "  \"\"\")\n",
        "\n",
        "func = mod.get_function(\"doublify\")\n",
        "\n",
        "# define the block and grid dimensions\n",
        "block = (16, 16, 1)\n",
        "grid = ((C + block[0] - 1) // block[0], (R + block[1] - 1) // block[1], 1)\n",
        "\n",
        "# now, we launch the kernel\n",
        "func(a_gpu, a_tid, a_bid, numpy.int32(R), numpy.int32(C), block=block, grid=grid)\n",
        "\n",
        "# create space in the host memory to receive results\n",
        "a_doubled = numpy.empty_like(a)\n",
        "a_tid_result = numpy.empty_like(b)\n",
        "a_bid_result = numpy.empty_like(c)\n",
        "\n",
        "# copy results from the GPU memory to the host memory\n",
        "cuda.memcpy_dtoh(a_doubled, a_gpu)\n",
        "cuda.memcpy_dtoh(b, a_tid)\n",
        "cuda.memcpy_dtoh(c, a_bid)\n",
        "\n",
        "# end timing\n",
        "print(round(time() - start_time, 8), 'seconds')\n",
        "\n",
        "# Optional: Print or verify results\n",
        "print(\"tid  bid\\n\")\n",
        "for i in range(min(10, len(b))):\n",
        "    s = \"\"\n",
        "    for j in range(min(10, len(b[i]))):\n",
        "        s += str(b[i, j]) + \"  \" + str(c[i, j]) + \"  \"\n",
        "    print(s)\n",
        "\n",
        "# Example comparison to verify doubling\n",
        "print((a * 2 == a_doubled).all())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbLnQy3oY_CE",
        "outputId": "650568d8-b100-4c4e-c998-afcf4bfbefbc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: device_allocation in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n",
            "/usr/local/lib/python3.10/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: module in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.03250861 seconds\n",
            "tid  bid\n",
            "\n",
            "0  256  1  256  2  256  3  256  4  256  5  256  6  256  7  256  8  256  9  256  \n",
            "16  256  17  256  18  256  19  256  20  256  21  256  22  256  23  256  24  256  25  256  \n",
            "32  256  33  256  34  256  35  256  36  256  37  256  38  256  39  256  40  256  41  256  \n",
            "48  256  49  256  50  256  51  256  52  256  53  256  54  256  55  256  56  256  57  256  \n",
            "64  256  65  256  66  256  67  256  68  256  69  256  70  256  71  256  72  256  73  256  \n",
            "80  256  81  256  82  256  83  256  84  256  85  256  86  256  87  256  88  256  89  256  \n",
            "96  256  97  256  98  256  99  256  100  256  101  256  102  256  103  256  104  256  105  256  \n",
            "112  256  113  256  114  256  115  256  116  256  117  256  118  256  119  256  120  256  121  256  \n",
            "128  256  129  256  130  256  131  256  132  256  133  256  134  256  135  256  136  256  137  256  \n",
            "144  256  145  256  146  256  147  256  148  256  149  256  150  256  151  256  152  256  153  256  \n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y08e9Alyz6MW"
      },
      "source": [
        "# Q1: Have you noticed that the array dimension, the grid shape and the offset calculated by each thread are all related? Increase the dimension of the arrays in both programs and play with the offset and with the `dim3`. Report what you understood about the distribution of threads and blocks and how the threads execute the kernel operations for each program."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtLhoNit7YYu"
      },
      "source": [
        "Knowing more about the device and the system..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3lyUC2C49Z4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0813f7a8-d9e8-4ab6-a917-4f707603d0de"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jun 21 15:54:56 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   78C    P0              33W /  70W |   1317MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kHJQB6M7fW3"
      },
      "source": [
        "Collecting device properties using `pycuda`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fbKTSVG5CoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0a7faf9-8d10-4fe5-cc8c-d646949b9235"
      },
      "source": [
        "import pycuda.driver as drv\n",
        "import pycuda.autoinit\n",
        "print(\"PyCUDA version:\", pycuda.VERSION_TEXT)\n",
        "print(\"Device(s) found:\", drv.Device.count())\n",
        "for ordinal in range(drv.Device.count()):\n",
        "  dev = drv.Device(ordinal)\n",
        "  print(\"Device number:\", ordinal, \"Device name:\", dev.name())\n",
        "  print(\"Compute capability: \", dev.compute_capability())\n",
        "  print(\"Max threads per block:\", dev.max_threads_per_block)\n",
        "  print(\"Max block_dim_x\", dev.MAX_BLOCK_DIM_X)\n",
        "  print(\"Max block_dim_y\", dev.MAX_BLOCK_DIM_Y)\n",
        "  print(\"Max block_dim_z\", dev.MAX_BLOCK_DIM_Z)\n",
        "  print(\"Max grid_dim_x\",dev.MAX_GRID_DIM_X)\n",
        "  print(\"Max grid_dim_y\",dev.MAX_GRID_DIM_Y)\n",
        "  print(\"Max grid_dim_z\",dev.MAX_GRID_DIM_Z)\n",
        "  print(\"Max total constant memory\",dev.TOTAL_CONSTANT_MEMORY)\n",
        "  print(\"Max warp size\",dev.WARP_SIZE)\n",
        "  print(dev.get_attributes())\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyCUDA version: 2024.1\n",
            "Device(s) found: 1\n",
            "Device number: 0 Device name: Tesla T4\n",
            "Compute capability:  (7, 5)\n",
            "Max threads per block: 1024\n",
            "Max block_dim_x 1024\n",
            "Max block_dim_y 1024\n",
            "Max block_dim_z 64\n",
            "Max grid_dim_x 2147483647\n",
            "Max grid_dim_y 65535\n",
            "Max grid_dim_z 65535\n",
            "Max total constant memory 65536\n",
            "Max warp size 32\n",
            "{pycuda._driver.device_attribute.ASYNC_ENGINE_COUNT: 3, pycuda._driver.device_attribute.CAN_MAP_HOST_MEMORY: 1, pycuda._driver.device_attribute.CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM: 1, pycuda._driver.device_attribute.CLOCK_RATE: 1590000, pycuda._driver.device_attribute.COMPUTE_CAPABILITY_MAJOR: 7, pycuda._driver.device_attribute.COMPUTE_CAPABILITY_MINOR: 5, pycuda._driver.device_attribute.COMPUTE_MODE: pycuda._driver.compute_mode.DEFAULT, pycuda._driver.device_attribute.COMPUTE_PREEMPTION_SUPPORTED: 1, pycuda._driver.device_attribute.CONCURRENT_KERNELS: 1, pycuda._driver.device_attribute.CONCURRENT_MANAGED_ACCESS: 1, pycuda._driver.device_attribute.DIRECT_MANAGED_MEM_ACCESS_FROM_HOST: 0, pycuda._driver.device_attribute.ECC_ENABLED: 1, pycuda._driver.device_attribute.GENERIC_COMPRESSION_SUPPORTED: 0, pycuda._driver.device_attribute.GLOBAL_L1_CACHE_SUPPORTED: 1, pycuda._driver.device_attribute.GLOBAL_MEMORY_BUS_WIDTH: 256, pycuda._driver.device_attribute.GPU_OVERLAP: 1, pycuda._driver.device_attribute.HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED: 1, pycuda._driver.device_attribute.HANDLE_TYPE_WIN32_HANDLE_SUPPORTED: 0, pycuda._driver.device_attribute.HANDLE_TYPE_WIN32_KMT_HANDLE_SUPPORTED: 0, pycuda._driver.device_attribute.HOST_NATIVE_ATOMIC_SUPPORTED: 0, pycuda._driver.device_attribute.INTEGRATED: 0, pycuda._driver.device_attribute.KERNEL_EXEC_TIMEOUT: 0, pycuda._driver.device_attribute.L2_CACHE_SIZE: 4194304, pycuda._driver.device_attribute.LOCAL_L1_CACHE_SUPPORTED: 1, pycuda._driver.device_attribute.MANAGED_MEMORY: 1, pycuda._driver.device_attribute.MAXIMUM_SURFACE1D_LAYERED_LAYERS: 2048, pycuda._driver.device_attribute.MAXIMUM_SURFACE1D_LAYERED_WIDTH: 32768, pycuda._driver.device_attribute.MAXIMUM_SURFACE1D_WIDTH: 32768, pycuda._driver.device_attribute.MAXIMUM_SURFACE2D_HEIGHT: 65536, pycuda._driver.device_attribute.MAXIMUM_SURFACE2D_LAYERED_HEIGHT: 32768, pycuda._driver.device_attribute.MAXIMUM_SURFACE2D_LAYERED_LAYERS: 2048, pycuda._driver.device_attribute.MAXIMUM_SURFACE2D_LAYERED_WIDTH: 32768, pycuda._driver.device_attribute.MAXIMUM_SURFACE2D_WIDTH: 131072, pycuda._driver.device_attribute.MAXIMUM_SURFACE3D_DEPTH: 16384, pycuda._driver.device_attribute.MAXIMUM_SURFACE3D_HEIGHT: 16384, pycuda._driver.device_attribute.MAXIMUM_SURFACE3D_WIDTH: 16384, pycuda._driver.device_attribute.MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS: 2046, pycuda._driver.device_attribute.MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH: 32768, pycuda._driver.device_attribute.MAXIMUM_SURFACECUBEMAP_WIDTH: 32768, pycuda._driver.device_attribute.MAXIMUM_TEXTURE1D_LAYERED_LAYERS: 2048, pycuda._driver.device_attribute.MAXIMUM_TEXTURE1D_LAYERED_WIDTH: 32768, pycuda._driver.device_attribute.MAXIMUM_TEXTURE1D_LINEAR_WIDTH: 268435456, pycuda._driver.device_attribute.MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH: 32768, pycuda._driver.device_attribute.MAXIMUM_TEXTURE1D_WIDTH: 131072, pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_ARRAY_HEIGHT: 32768, pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES: 2048, pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_ARRAY_WIDTH: 32768, pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_GATHER_HEIGHT: 32768, pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_GATHER_WIDTH: 32768, pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_HEIGHT: 65536, pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_LINEAR_HEIGHT: 65000, pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_LINEAR_PITCH: 2097120, pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_LINEAR_WIDTH: 131072, pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT: 32768, pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH: 32768, pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_WIDTH: 131072, pycuda._driver.device_attribute.MAXIMUM_TEXTURE3D_DEPTH: 16384, pycuda._driver.device_attribute.MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE: 32768, pycuda._driver.device_attribute.MAXIMUM_TEXTURE3D_HEIGHT: 16384, pycuda._driver.device_attribute.MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE: 8192, pycuda._driver.device_attribute.MAXIMUM_TEXTURE3D_WIDTH: 16384, pycuda._driver.device_attribute.MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE: 8192, pycuda._driver.device_attribute.MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS: 2046, pycuda._driver.device_attribute.MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH: 32768, pycuda._driver.device_attribute.MAXIMUM_TEXTURECUBEMAP_WIDTH: 32768, pycuda._driver.device_attribute.MAX_BLOCKS_PER_MULTIPROCESSOR: 16, pycuda._driver.device_attribute.MAX_BLOCK_DIM_X: 1024, pycuda._driver.device_attribute.MAX_BLOCK_DIM_Y: 1024, pycuda._driver.device_attribute.MAX_BLOCK_DIM_Z: 64, pycuda._driver.device_attribute.MAX_GRID_DIM_X: 2147483647, pycuda._driver.device_attribute.MAX_GRID_DIM_Y: 65535, pycuda._driver.device_attribute.MAX_GRID_DIM_Z: 65535, pycuda._driver.device_attribute.MAX_PERSISTING_L2_CACHE_SIZE: 0, pycuda._driver.device_attribute.MAX_PITCH: 2147483647, pycuda._driver.device_attribute.MAX_REGISTERS_PER_BLOCK: 65536, pycuda._driver.device_attribute.MAX_REGISTERS_PER_MULTIPROCESSOR: 65536, pycuda._driver.device_attribute.MAX_SHARED_MEMORY_PER_BLOCK: 49152, pycuda._driver.device_attribute.MAX_SHARED_MEMORY_PER_BLOCK_OPTIN: 65536, pycuda._driver.device_attribute.MAX_SHARED_MEMORY_PER_MULTIPROCESSOR: 65536, pycuda._driver.device_attribute.MAX_THREADS_PER_BLOCK: 1024, pycuda._driver.device_attribute.MAX_THREADS_PER_MULTIPROCESSOR: 1024, pycuda._driver.device_attribute.MEMORY_CLOCK_RATE: 5001000, pycuda._driver.device_attribute.MEMORY_POOLS_SUPPORTED: 1, pycuda._driver.device_attribute.MULTIPROCESSOR_COUNT: 40, pycuda._driver.device_attribute.MULTI_GPU_BOARD: 0, pycuda._driver.device_attribute.MULTI_GPU_BOARD_GROUP_ID: 0, pycuda._driver.device_attribute.PAGEABLE_MEMORY_ACCESS: 0, pycuda._driver.device_attribute.PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES: 0, pycuda._driver.device_attribute.PCI_BUS_ID: 0, pycuda._driver.device_attribute.PCI_DEVICE_ID: 4, pycuda._driver.device_attribute.PCI_DOMAIN_ID: 0, pycuda._driver.device_attribute.READ_ONLY_HOST_REGISTER_SUPPORTED: 1, pycuda._driver.device_attribute.RESERVED_SHARED_MEMORY_PER_BLOCK: 0, pycuda._driver.device_attribute.SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO: 32, pycuda._driver.device_attribute.STREAM_PRIORITIES_SUPPORTED: 1, pycuda._driver.device_attribute.SURFACE_ALIGNMENT: 512, pycuda._driver.device_attribute.TCC_DRIVER: 0, pycuda._driver.device_attribute.TEXTURE_ALIGNMENT: 512, pycuda._driver.device_attribute.TEXTURE_PITCH_ALIGNMENT: 32, pycuda._driver.device_attribute.TOTAL_CONSTANT_MEMORY: 65536, pycuda._driver.device_attribute.UNIFIED_ADDRESSING: 1, pycuda._driver.device_attribute.WARP_SIZE: 32}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2: inspect the nvidia-smi command and check how you can obtain the same information or more detailed information using the command line."
      ],
      "metadata": {
        "id": "oTk6fkGLlaCy"
      }
    }
  ]
}